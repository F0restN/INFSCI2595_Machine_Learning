---
title: "Drake_Zhou_Part_02_D"
author: "Drake Zhou"
date: "4/25/2022"
output: 
  html_document: 
    toc: true
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

## Package Import

```{r, message=FALSE}
library(tidyverse)
library(caret)
library(coefplot)
library(rstanarm)
library(splines)
library(kernlab)
```

## Data Import

```{r}
df_all <- readr::read_csv("final_project_train.csv", col_names = TRUE)

standard_df_all <- df_all %>% 
  select(starts_with("x"), response, region, customer)
```

# Training model

```{r, eval=TRUE}
my_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
my_metric <- "RMSE"
```

• Linear models:• All categorical and continuous inputs - linear additive features
  - All pairwise interactions of continuous inputs, include additive categorical features
  - The 2 models selected from iiA)
• Regularized regression with Elastic net
  - All pairwise interactions of continuous inputs, include additive categorical features
  - The more complex of the 2 models selected from iiA)
• Neural network
• Random forest
• Gradient boosted tree
• 2 methods of your choice that we did not explicitly discuss in lecture

## Linear models:

### All categorical and continuous inputs - linear additive features

```{r }
set.seed(1024)
fit_lm_add <- train(response ~ .,
                  data = standard_df_all,
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_add
```

### All pairwise interactions of continuous inputs, include additive categorical features

```{r, message=FALSE, warning=FALSE}
set.seed(1024)
fit_lm_pair <- train(response ~ region + customer + (.)^2,
                  data = standard_df_all,
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_pair
```

### The 2 models selected from iiA)

```{r}
set.seed(1024)
fit_lm_mod1 <- train(response ~ region + customer,
                  data = standard_df_all,
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_mod1
```

```{r}
set.seed(1024)
fit_lm_mod2 <- train(response ~ .,
                  data = standard_df_all[c(-36, -35)],
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_mod2
```

## Elastic net

### All pairwise interactions of continuous inputs, include additive categorical features

```{r , message=FALSE, warning=FALSE}
set.seed(1024)
fit_enet_pair_warmup <- train(response ~ region + customer + (.)^2,
                    data = standard_df_all,
                    method = "glmnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

```

```{r}
plot(fit_enet_pair_warmup)
```

```{r, message=FALSE, warning=FALSE}
# optimize an elnet regression
tune_grid <- expand.grid(.alpha = seq(0, 0.2, length.out = 5),  
                        .lambda = seq(0, 0.2, length.out = 100))

set.seed(1024)
fit_enet_pair_add <- train(response ~ region + customer + (.)^2,
                    data = standard_df_all,
                    method = "glmnet",
                    metric = my_metric,
                    tuneGrid = tune_grid,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

```

```{r}
plot(fit_enet_pair_add)
```

### The more complex of the 2 models selected from iiA)

1. All pair-wise

```{r, message=FALSE, warning=FALSE}
set.seed(1024)
fit_enet_pair <- train(response ~ (.)^2,
                    data = standard_df_all,
                    method = "glmnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)
```

```{r}
plot(fit_enet_pair)
fit_enet_pair
```

2. Model with polynomial

```{r , warning=FALSE, message=FALSE}

formula_mod7 <- response ~ poly(xb_01, 2)+poly(xb_02, 2)+ poly(xb_03,2)+poly(xn_01,2)+poly(xn_02,2)+poly(xn_03,2)+poly(xa_01,2)+poly(xa_02,2)+poly(xa_03,2)+poly(xb_04,2)+poly(xb_05,2)+poly(xb_06,2)+poly(xb_07,2)+poly(xb_08,2)+poly(xn_04,2)+poly(xn_05,2)+poly(xn_06,2)+poly(xn_07,2)+poly(xn_08,2)+poly(xa_04,2)+poly(xa_05,2)+poly(xa_06,2)+poly(xa_07,2)+poly(xa_08,2)+poly(xw_01,2)+poly(xw_02,2)+poly(xw_03,2)+poly(xs_01,2)+poly(xs_02,2)+poly(xs_03,2)+poly(xs_04,2)+poly(xs_05,2)+poly(xs_06,2)+region+customer

set.seed(1024)
fit_enet_mod7 <- train(formula_mod7,
                       data = standard_df_all,
                       method = "glmnet",
                       metric = my_metric,
                       preProcess = c("center", "scale"),
                       trControl = my_ctrl)

fit_enet_mod7

```

3. Model with natural spline

```{r , warning=FALSE, message=FALSE}
formula_mod9 <- response ~ ns(xb_01, 3)+ns(xb_03, 3)+ ns(xb_03,3)+ns(xn_01,3)+ns(xn_03,3)+ns(xn_03,3)+ns(xa_01,3)+ns(xa_03,3)+ns(xa_03,3)+ns(xb_04,3)+ns(xb_05,3)+ns(xb_06,3)+ns(xb_07,3)+ns(xb_08,3)+ns(xn_04,3)+ns(xn_05,3)+ns(xn_06,3)+ns(xn_07,3)+ns(xn_08,3)+ns(xa_04,3)+ns(xa_05,3)+ns(xa_06,3)+ns(xa_07,3)+ns(xa_08,3)+ns(xw_01,3)+ns(xw_03,3)+ns(xw_03,3)+ns(xs_01,3)+ns(xs_03,3)+ns(xs_03,3)+ns(xs_04,3)+ns(xs_05,3)+ns(xs_06,3)+region+customer

set.seed(1024)
fit_enet_mod9 <- train(formula_mod9,
                       data = standard_df_all,
                       method = "glmnet",
                       metric = my_metric,
                       preProcess = c("center", "scale"),
                       trControl = my_ctrl)

fit_enet_mod9

```

## Neural network

Fit the model as a warmup, then use parameters to further tune this model.

```{r}
set.seed(1024)

fit_nnet_warmup <- train(response ~ .,
                    data = standard_df_all,
                    method = "nnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl,
                    trace = FALSE,
                    linout = TRUE)

fit_nnet_warmup
```

Visualize the result and find tuning parameters

```{r}
plot(fit_nnet_warmup, xTrans = log)
```


```{r }
tune_grid_neural <- expand.grid(size = c(1:5, 10),
                                decay = c(0, 0.05, 0.1, 1, 2))

fit_nnet_tune <- train(response ~ .,
                    data = standard_df_all,
                    method = "nnet",
                    metric = my_metric,
                    tuneGrid = tune_grid_neural,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl,
                    trace = FALSE,
                    linout = TRUE)
```

Check the result

```{r}
fit_nnet_tune
plot(fit_nnet_tune, xTrans = log)
```


## Random forest

```{r }
set.seed(12341)
fit_rf_tune <- train(response ~ .,
                     data = standard_df_all,
                     method = "rf",
                     metric = "RMSE",
                     trControl = my_ctrl,
                     tuneGrid = expand.grid(mtry = seq(2, 8, by = 1)),
                     importance = TRUE)

fit_rf_tune
```

## Gradient Boosted Tree

```{r }
set.seed(12341)
fit_gbm_warmup <- train(response ~ .,
                      data = standard_df_all,
                      method = "gbm",
                      metric = my_metric,
                      trControl = my_ctrl,
                      verbose=FALSE)

fit_gbm_warmup
```

Visualize the result and finding best tuning parameters

```{r}
plot(fit_gbm_warmup)
```

Tuning GBM model

```{r }
gbm_grid <- expand.grid(n.trees = c(100, 150, 300, 500, 750, 1000),
                        shrinkage = c(0.01, 0.1),
                        interaction.depth = fit_gbm_warmup$bestTune$interaction.depth,
                        n.minobsinnode = fit_gbm_warmup$bestTune$n.minobsinnode)

set.seed(12341)
fit_gbm_tune <- train(response ~ .,
                      data = standard_df_all,
                      method = "gbm",
                      metric = my_metric,
                      tuneGrid = gbm_grid,
                      trControl = my_ctrl,
                      verbose=FALSE)
```

```{r }
fit_gbm_tune
plot(fit_gbm_tune)
```

• 2 methods of your choice that we did not explicitly discuss in lecture

## PLS

```{r , warning=FALSE, message=FALSE}
pls_grid <- expand.grid(ncomp = 1:5)
set.seed(2022)
fit_pls_tune <- train(response ~ .,
                 data = standard_df_all,
                 method = "pls",
                 metric = my_metric,
                 tuneGrid = pls_grid,
                 preProcess = c("center", "scale"),
                 trControl = my_ctrl)

fit_pls_tune
```

Visualize the result and find best tuning parameters

```{r}
plot(fit_pls_tune)
```

## SVM

```{r }
set.seed(1024)
fit_svm_tune <- train(response ~ .,
                 data = standard_df_all,
                 method = "svmRadial",
                 metric = my_metric,
                 preProcess = c("center", "scale"),
                 trControl = my_ctrl)

fit_svm_tune
```

# Comparasion

```{r}
my_performance <- resamples(list(LM_1 = fit_lm_add,
                                 #LM_2 = fit_lm_pair,
                                 LM_MOD1 = fit_lm_mod1,
                                 LM_MOD2 = fit_lm_mod2,
                                 ENET_PAIR = fit_enet_pair,
                                 ENET_PAIR_ADD = fit_enet_pair_add,
                                 ENET_MOD7 = fit_enet_mod7,
                                 ENET_MOD9 = fit_enet_mod9,
                                 GBM_W = fit_gbm_warmup,
                                 GBN_T = fit_gbm_tune,
                                 NNET_W = fit_nnet_warmup,
                                 NNET_T = fit_nnet_tune,
                                 RF = fit_rf_tune,
                                 SVM = fit_svm_tune,
                                 PLS_T = fit_pls_tune))
```

```{r}
dotplot(my_performance, metric = "RMSE", scales = 'free')
```

```{r}
dotplot(my_performance, metric = "Rsquared")
```

```{r, eval=TRUE}
my_performance_resample <- my_performance$values %>% tibble::as_tibble() %>% 
  pivot_longer(!c("Resample")) %>% 
  tidyr::separate(name,
                  c("model_name", "metric_name"),
                  sep = "~")

my_performance_resample %>% 
  ggplot(mapping = aes(x = model_name, y = value)) +
  geom_point() +
  stat_summary(fun.data = "mean_se",
               color = "red",
               fun.args = list(mult = 1)) +
  coord_flip() +
  facet_grid( . ~ metric_name, scales = "free_x") +
  theme_bw()
```

Which model is the best?

```{text}
According to figures above, All pairwise interactions of continuous inputs, include additive categorical features with elastic net regularization is the best model. Which is : fit_enet_pair_add
```

# Variables Importance

```{r}
plot(varImp(fit_enet_pair_add), top = 20)
```

# Export Models

```{r}

fit_lm_add %>% readr::write_rds("models/fit_reg_lm_add.rds")
fit_lm_pair %>% readr::write_rds("models/fit_reg_lm_pair.rds")
fit_lm_mod1 %>% readr::write_rds("models/fit_reg_lm_mod1.rds")
fit_lm_mod2 %>% readr::write_rds("models/fit_reg_lm_mod2.rds")
fit_enet_pair_add %>% readr::write_rds("models/fit_reg_enet_pair.rds")
fit_enet_pair %>% readr::write_rds("models/fit_reg_enet_pair_all.rds")
fit_enet_mod7 %>% readr::write_rds("models/fit_reg_enet_mod7.rds")
fit_enet_mod9 %>% readr::write_rds("models/fit_reg_enet_mod9.rds")
fit_gbm_warmup %>% readr::write_rds("models/fit_reg_gbm_warmup.rds")
fit_gbm_tune %>% readr::write_rds("models/fit_reg_gbm_tune.rds")
fit_nnet_warmup %>% readr::write_rds("models/fit_reg_nnet_warmup.rds")
fit_nnet_tune %>% readr::write_rds("models/fit_reg_nnet_tune.rds")
fit_rf_tune %>% readr::write_rds("models/fit_reg_rf_tune.rds")
fit_svm_tune %>% readr::write_rds("models/fit_reg_svm_tune.rds")
fit_pls_tune %>% readr::write_rds("models/fit_reg_pls_tune.rds")

```

